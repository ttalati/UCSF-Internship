{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries.\n",
    "\n",
    "- BeautifulSoup is used for web scrapping purposes\n",
    "\n",
    "- requests, urllib, re, ssl, calender used for making api request\n",
    "\n",
    "- json used to deal with json format\n",
    "\n",
    "- numpy used for math\n",
    "\n",
    "- pandas used for dealing with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, urllib, re, ssl, json, calendar\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Request Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_req_url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=100&sort=relevance&term=\"\n",
    "keyword = \"KEYWORD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the keyword required: \n",
      "covid\n"
     ]
    }
   ],
   "source": [
    "keyword = str(input(\"Enter the keyword required: \\n\"))\n",
    "api_req_url = api_req_url + keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to verify your HTTP Certificate and then make request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_create_unverified_https_context = ssl._create_unverified_context\n",
    "request = urllib.request.urlopen(api_req_url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to be able to access JSON data and parse specific things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['32650110',\n",
       " '32495147',\n",
       " '32461198',\n",
       " '32416116',\n",
       " '32620118',\n",
       " '32409232',\n",
       " '32387968',\n",
       " '32620311',\n",
       " '32605250',\n",
       " '32571224',\n",
       " '32356900',\n",
       " '32344321',\n",
       " '32558002',\n",
       " '32525022',\n",
       " '32311448',\n",
       " '32522141',\n",
       " '32460829',\n",
       " '32642086',\n",
       " '32617220',\n",
       " '32579306',\n",
       " '32566555',\n",
       " '32479979',\n",
       " '32438898',\n",
       " '32392129',\n",
       " '32220208',\n",
       " '32340094',\n",
       " '32259124',\n",
       " '32192278',\n",
       " '32649990',\n",
       " '32649840',\n",
       " '32649738',\n",
       " '32648989',\n",
       " '32648093',\n",
       " '32641133',\n",
       " '32641059',\n",
       " '32635422',\n",
       " '32634716',\n",
       " '32634029',\n",
       " '32634023',\n",
       " '32631442',\n",
       " '32624089',\n",
       " '32623038',\n",
       " '32622888',\n",
       " '32621617',\n",
       " '32621013',\n",
       " '32617983',\n",
       " '32617807',\n",
       " '32615936',\n",
       " '32615802',\n",
       " '32613637',\n",
       " '32610371',\n",
       " '32609908',\n",
       " '32609906',\n",
       " '32598204',\n",
       " '32588779',\n",
       " '32568726',\n",
       " '32538295',\n",
       " '32511791',\n",
       " '32500409',\n",
       " '32490559',\n",
       " '32469164',\n",
       " '32452556',\n",
       " '32447742',\n",
       " '32440883',\n",
       " '32437587',\n",
       " '32432994',\n",
       " '32430957',\n",
       " '32417124',\n",
       " '32413736',\n",
       " '32408453',\n",
       " '32406001',\n",
       " '32405101',\n",
       " '32402910',\n",
       " '32397915',\n",
       " '32394237',\n",
       " '32388129',\n",
       " '32386882',\n",
       " '32376392',\n",
       " '32366160',\n",
       " '32363809',\n",
       " '32361529',\n",
       " '32361443',\n",
       " '32360780',\n",
       " '32360108',\n",
       " '32358057',\n",
       " '32348166',\n",
       " '32339844',\n",
       " '32335167',\n",
       " '32334062',\n",
       " '32331845',\n",
       " '32325421',\n",
       " '32320825',\n",
       " '32306864',\n",
       " '32306118',\n",
       " '32294485',\n",
       " '32281668',\n",
       " '32278175',\n",
       " '32278147',\n",
       " '32259279',\n",
       " '32186894']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(request)\n",
    "\n",
    "article_ids = data['esearchresult']['idlist']\n",
    "article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements(soup_object):\n",
    "    \n",
    "    authors = getAuthors(soup_object)\n",
    "    title = getTitle(soup_object)\n",
    "    abstract = getAbstract(soup_object)\n",
    "    pubmed_id = getID(soup_object)\n",
    "    \n",
    "    journal_volume = \"\"\n",
    "    journal_title = \"\"\n",
    "    journal_date = \"\"\n",
    "    \n",
    "    if soup_object.find(\"JournalIssue\"):\n",
    "        if soup_object.find(\"JournalIssue\").find(\"Volume\"):\n",
    "            journal_volume = soup_object.find(\"JournalIssue\").find(\"Volume\").text\n",
    "    if soup_object.find(\"Title\"):\n",
    "        journal_title = soup_object.find(\"Title\").text\n",
    "    if soup_object.find(\"JournalIssue\").find(\"Year\") and soup_object.find(\"JournalIssue\").find(\"Month\") and soup_object.find(\"JournalIssue\").find(\"Day\"):\n",
    "        journal_date = soup_object.find(\"JournalIssue\").find(\"Year\").text + \" \" + soup_object.find(\"JournalIssue\").find(\"Month\").text + \" \" + soup_object.find(\"JournalIssue\").find(\"Day\").text\n",
    "    \n",
    "    row_list = []\n",
    "    row_list.append(authors)\n",
    "    row_list.append(title)\n",
    "    row_list.append(journal_title)\n",
    "    row_list.append(journal_volume)\n",
    "    row_list.append(journal_date)\n",
    "    row_list.append(pubmed_id)\n",
    "    row_list.append(abstract)\n",
    "    \n",
    "    return row_list\n",
    "    \n",
    "def getAuthors(soup_object):\n",
    "    authors = \"\"\n",
    "    authorlist = soup_object.find(\"AuthorList\")\n",
    "    ##print(authorlist)\n",
    "    \n",
    "    if authorlist: \n",
    "        for i in range(len(authorlist.find_all(\"LastName\"))):\n",
    "            firstName = authorlist.find_all(\"ForeName\")[i].text\n",
    "            authors = authors + firstName\n",
    "            lastName = authorlist.find_all(\"LastName\")[i].text\n",
    "            authors = authors + \" \" + lastName\n",
    "            \n",
    "            if i != len(authorlist.find_all(\"LastName\")) - 1:\n",
    "                authors = authors + \", \"\n",
    "    print(authors)\n",
    "    return authors\n",
    "\n",
    "def getTitle(soup_object):\n",
    "    title = \"\"\n",
    "    if soup_object.find(\"AricleTitle\"):\n",
    "        title_element = soup_object.find(\"ArticleTitle\")\n",
    "        title = title_element.text\n",
    "\n",
    "    print(title)\n",
    "    return title\n",
    "    \n",
    "def getAbstract(soup_object):\n",
    "    abstract = \"\"\n",
    "    if soup_object.find(\"AbstractText\"):\n",
    "        abstract = soup_object.find(\"AbstractText\").text\n",
    "    print(abstract)\n",
    "    return abstract\n",
    "\n",
    "def getID(soup_object):\n",
    "    idInfo = \"\"\n",
    "    if soup_object.find(\"PMID\"):\n",
    "        idInfo = \"PMID: \"\n",
    "        idInfo += soup_object.find(\"PMID\").text\n",
    "    return idInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominique Salmon, Sophie Bartier, Charlotte Hautefort, Yann Nguyen, Jérôme Nevoux, Anne-Laure Hamel, Yohan Camhi, Florence Canouï-Poitrine, Benjamin Verillaud, Dorsaf Slama, Stephanie Haim-Boukobza, Elise Sourdeau, Delphine Cantin, Alain Corré, Agnes Bryn, Nicolas Etienne, Flore Rozenberg, Richard Layese, Jean-François Papon, Emilie Bequignon\n",
      "\n",
      "To determine the frequency of SARS-CoV-2 positive samples in a subset of patients consulting for primarily isolated acute (<7 days) loss of smell and to assess the diagnostic accuracy of olfactory/gustatory dysfunction for COVID-19 diagnosis in the overall population tested for COVID-19 in the same period.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-bf4bfc6f1aae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msoup_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mone_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mall_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-b14e3f0e08e5>\u001b[0m in \u001b[0;36mget_elements\u001b[1;34m(soup_object)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpubmed_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetID\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mjournal_volume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"JournalIssue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Volume\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mjournal_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mjournal_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"JournalIssue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Year\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msoup_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"JournalIssue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Month\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msoup_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"JournalIssue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Day\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "all_articles = []\n",
    "for articleID in article_ids:\n",
    "   \n",
    "    get_url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=\"\n",
    "    get_url = get_url + articleID\n",
    "    \n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    response = requests.get(get_url)\n",
    "\n",
    "    soup_object = BeautifulSoup(response.content, \"xml\")\n",
    "    one_article = get_elements(soup_object)\n",
    "    all_articles.append(one_article)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
