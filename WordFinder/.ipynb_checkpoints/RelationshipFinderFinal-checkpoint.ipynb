{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries.\n",
    "\n",
    "- BeautifulSoup is used for web scrapping purposes\n",
    "\n",
    "- requests, urllib, and ssl are used for making api request\n",
    "\n",
    "- json used to deal with json format\n",
    "\n",
    "- pandas used for dealing with data and exporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, urllib, ssl, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_req_url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=100&sort=relevance&term=\"\n",
    "keyword = \"KEYWORD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the keyword required: \n",
      "immport\n"
     ]
    }
   ],
   "source": [
    "keyword = str(input(\"Enter the keyword required: \\n\"))\n",
    "api_req_url = api_req_url + keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses PUBMED API to get all the articles returned from the simple keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_create_unverified_https_context = ssl._create_unverified_context\n",
    "request = urllib.request.urlopen(api_req_url).read()\n",
    "\n",
    "data = json.loads(request)\n",
    "\n",
    "article_ids = data['esearchresult']['idlist']\n",
    "len(article_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve_citations gets all the papers that have cited current paper.\n",
    "\n",
    "find_citations returns all the ids of papers that a certain paper cites.\n",
    "\n",
    "All ids are PMIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_citations(id_str):\n",
    "    api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_citedin&id=\"  \n",
    "    api_url = api_url + id_str\n",
    "    ##print(api_url)\n",
    "    \n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    soup_object = BeautifulSoup(response.content, \"xml\")\n",
    "    \n",
    "    linked_id_list = []\n",
    "    \n",
    "    link_list = soup_object.find_all(\"Link\")\n",
    "    for link in link_list:\n",
    "        if \"Id\" in str(link):\n",
    "            text = link.text.replace(\"\\n\", \"\")\n",
    "            linked_id_list.append(text)\n",
    "    print(linked_id_list)\n",
    "    return linked_id_list\n",
    "##this finds all the articles one has cited \n",
    "def find_citations(id_str):\n",
    "    api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_refs&id=\"\n",
    "    api_url = api_url + id_str\n",
    "    ##print(api_url)\n",
    "    \n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    soup_object = BeautifulSoup(response.content, \"xml\")\n",
    "    \n",
    "    linked_id_list = []\n",
    "    \n",
    "    link_list = soup_object.find_all(\"Link\")\n",
    "    for link in link_list:\n",
    "        if \"Id\" in str(link):\n",
    "            text = link.text.replace(\"\\n\", \"\")\n",
    "            linked_id_list.append(text)\n",
    "    print(linked_id_list)\n",
    "    return linked_id_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just two examples of the method being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30357391', '29761457', '29485622', '28365732', '27128319', '26467479', '25348409', '18940862', '15760272', '824647']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['30357391',\n",
       " '29761457',\n",
       " '29485622',\n",
       " '28365732',\n",
       " '27128319',\n",
       " '26467479',\n",
       " '25348409',\n",
       " '18940862',\n",
       " '15760272',\n",
       " '824647']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_citations(\"32283555\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['33014809', '33013820', '33005178', '32994819', '32989393', '32983989', '32974146', '32950055', '32884943', '32879664', '32873319', '32854645', '32850416', '32850407', '32793281', '32789283', '32717640', '32699529', '32666718', '32624696', '32588760', '32569190', '32564470', '32537435', '32411773', '32411181', '32355728', '32352015', '32351547', '32322240', '32309184', '32296685', '32296631', '32291385', '32286381', '32219223', '32191631', '32179831', '32158466', '32143735', '32117435', '32081859', '32046766', '32039832', '32028264', '32019546', '32004409', '32002550', '31929113', '31892734', '31865910', '31842801', '31824866', '31821170', '31741756', '31695415', '31633216', '31416125', '31414702', '31371577', '31277152', '31143520', '30945466', '30852461', '30832680', '30661062', '30622534', '30594555', '30516029', '30398470', '30357391', '30304689', '30169745', '30153793', '30067990', '29921729', '29912209', '29769441', '29688354', '29490162', '29485622', '29346583', '29325141', '29322913', '29259604', '29228196', '29163494', '29144493', '29141660', '28968677', '28959737', '28925997', '28687838', '28652380', '28637268', '28485405', '28369768', '28365732', '28264647', '28134706', '28057685', '27986392', '27916977', '27899649', '27863400', '27851916', '27792773', '27766961', '27627881', '27515123', '27377652', '27376489', '27189372', '27130330', '27052692', '26776196', '26682988', '26661719', '26657882', '26560699', '26387933', '26362267', '26284066', '26112029', '26107960', '25964458', '25858860', '25433677', '25418588']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['33014809',\n",
       " '33013820',\n",
       " '33005178',\n",
       " '32994819',\n",
       " '32989393',\n",
       " '32983989',\n",
       " '32974146',\n",
       " '32950055',\n",
       " '32884943',\n",
       " '32879664',\n",
       " '32873319',\n",
       " '32854645',\n",
       " '32850416',\n",
       " '32850407',\n",
       " '32793281',\n",
       " '32789283',\n",
       " '32717640',\n",
       " '32699529',\n",
       " '32666718',\n",
       " '32624696',\n",
       " '32588760',\n",
       " '32569190',\n",
       " '32564470',\n",
       " '32537435',\n",
       " '32411773',\n",
       " '32411181',\n",
       " '32355728',\n",
       " '32352015',\n",
       " '32351547',\n",
       " '32322240',\n",
       " '32309184',\n",
       " '32296685',\n",
       " '32296631',\n",
       " '32291385',\n",
       " '32286381',\n",
       " '32219223',\n",
       " '32191631',\n",
       " '32179831',\n",
       " '32158466',\n",
       " '32143735',\n",
       " '32117435',\n",
       " '32081859',\n",
       " '32046766',\n",
       " '32039832',\n",
       " '32028264',\n",
       " '32019546',\n",
       " '32004409',\n",
       " '32002550',\n",
       " '31929113',\n",
       " '31892734',\n",
       " '31865910',\n",
       " '31842801',\n",
       " '31824866',\n",
       " '31821170',\n",
       " '31741756',\n",
       " '31695415',\n",
       " '31633216',\n",
       " '31416125',\n",
       " '31414702',\n",
       " '31371577',\n",
       " '31277152',\n",
       " '31143520',\n",
       " '30945466',\n",
       " '30852461',\n",
       " '30832680',\n",
       " '30661062',\n",
       " '30622534',\n",
       " '30594555',\n",
       " '30516029',\n",
       " '30398470',\n",
       " '30357391',\n",
       " '30304689',\n",
       " '30169745',\n",
       " '30153793',\n",
       " '30067990',\n",
       " '29921729',\n",
       " '29912209',\n",
       " '29769441',\n",
       " '29688354',\n",
       " '29490162',\n",
       " '29485622',\n",
       " '29346583',\n",
       " '29325141',\n",
       " '29322913',\n",
       " '29259604',\n",
       " '29228196',\n",
       " '29163494',\n",
       " '29144493',\n",
       " '29141660',\n",
       " '28968677',\n",
       " '28959737',\n",
       " '28925997',\n",
       " '28687838',\n",
       " '28652380',\n",
       " '28637268',\n",
       " '28485405',\n",
       " '28369768',\n",
       " '28365732',\n",
       " '28264647',\n",
       " '28134706',\n",
       " '28057685',\n",
       " '27986392',\n",
       " '27916977',\n",
       " '27899649',\n",
       " '27863400',\n",
       " '27851916',\n",
       " '27792773',\n",
       " '27766961',\n",
       " '27627881',\n",
       " '27515123',\n",
       " '27377652',\n",
       " '27376489',\n",
       " '27189372',\n",
       " '27130330',\n",
       " '27052692',\n",
       " '26776196',\n",
       " '26682988',\n",
       " '26661719',\n",
       " '26657882',\n",
       " '26560699',\n",
       " '26387933',\n",
       " '26362267',\n",
       " '26284066',\n",
       " '26112029',\n",
       " '26107960',\n",
       " '25964458',\n",
       " '25858860',\n",
       " '25433677',\n",
       " '25418588']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_citations(\"24791905\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes all the initial articles at that were found in the beginning and stores the ids of all their references in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dict = {}\n",
    "for id_str in article_ids:\n",
    "    citation_list = find_citations(id_str)\n",
    "    print(id_str)\n",
    "    main_dict[str(id_str)] = citation_list\n",
    "    ##retrieve_citations(\"24791905\")\n",
    "    \n",
    "print(main_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes due to the format of the article the API does not return references so this is just an indicator of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_citing = []\n",
    "for article in main_dict:\n",
    "    cited = False\n",
    "    for cited in main_dict[article]:\n",
    "        if(cited == \"24791905\" or cited == \"29485622\"):\n",
    "            cited = True\n",
    "    if(not cited):\n",
    "        print(article)\n",
    "        not_citing.append(article)   \n",
    "        \n",
    "len(not_citing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stores reference dictionary into json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('references.txt', 'w') as json_file:\n",
    "  json.dump(main_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the ids into api url calls, which are then stored to use javascript in the Zotero console to extract the full text pdfs. \n",
    "\n",
    "Note the current code calls on retrieve citations since study was done with all the articles that cited \"ImmPort: disseminating data to the public for the future of immunology.\" \n",
    "\n",
    "However the method call can be changed if the approach of studying from the citations of the articles that come up from the key word search is desired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeURLS(article_ids):\n",
    "    global_string = \"\"\n",
    "    count = 0\n",
    "    for id_str in article_ids:\n",
    "        if not count == (len(article_ids) - 1):\n",
    "            global_string = global_string + \"https://pubmed.ncbi.nlm.nih.gov/\" + id_str + \"/\" + \"\\n\"\n",
    "        else:\n",
    "           global_string = global_string + \"https://pubmed.ncbi.nlm.nih.gov/\" + id_str + \"/\"\n",
    "        count = count + 1\n",
    "    return global_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_urls = open(\"test_id.txt\",\"w+\")\n",
    "reference_urls.write(makeURLS(retrieve_citations(\"24791905\")))\n",
    "reference_urls.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This would be the JS code that would run in zotero console with the path to the file that is made in the block of code above.\n",
    "\n",
    "var path = '';\n",
    "var urls = Zotero.File.getContents(path).split('\\n').map(url => url);\n",
    "await Zotero.HTTP.processDocuments(\n",
    "urls,\n",
    "\n",
    "    async function (doc) {\n",
    "        var translate = new Zotero.Translate.Web();\n",
    "        translate.setDocument(doc);\n",
    "        var translators = await translate.getTranslators();\n",
    "        if (translators.length) {\n",
    "            translate.setTranslator(translators[0]);\n",
    "            try {\n",
    "                await translate.translate();\n",
    "                return;\n",
    "                }\n",
    "            catch (e) {}\n",
    "       }\n",
    "       await ZoteroPane.addItemFromDocument(doc);\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after the full texts have been stored on the computer, they can be parsed to do analysis. In this notebook, only the OCR is used to extract text. Other notebooks in the repository contain the use of other methods (which are faster but less accurate) \n",
    "\n",
    "If for whatever reason they want to be seen please look at LargeScrape.ipynb and PDFScraper.ipynb\n",
    "\n",
    "\n",
    "Additional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract as pt\n",
    "import pdf2image\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates all the directory names given the starting directory of the area with the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths(start_file):\n",
    "    directory_list = []\n",
    "    directory = start_file\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            print(os.path.join(directory, filename))\n",
    "            directory_list.append(directory+\"\\\\\"+filename)\n",
    "            print(directory_list)\n",
    "    return directory_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = find_paths(\"ENTER DIRECTORY HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getText uses Teseract, which uses OCR, converting each page into an image before extracting text using OCR.\n",
    "\n",
    "getPaths returns the list of article PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(pdf):\n",
    "    try:\n",
    "        full_text = \"\"\n",
    "        pages = pdf2image.convert_from_path(pdf_path=pdf, dpi=200, size=(1654,2340))   \n",
    "        for i in range(len(pages)):\n",
    "            pages[i].save('Images\\\\imagenum' + str(i) + '.jpg')\n",
    "         \n",
    "        print(str(len(pages))) \n",
    "        print(\"\\n\")\n",
    "     \n",
    "        for i in range(len(pages)):\n",
    "            content = pt.image_to_string(pages[i], lang='eng')\n",
    "            print(content)\n",
    "            full_text = full_text + \" \" + content\n",
    "        return full_text\n",
    "    except Exception:\n",
    "        return \"Text For This File Could Not be Read\"\n",
    "\n",
    "def getPaths(start_file):\n",
    "    path_list = []\n",
    "    directory = start_file\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "         if filename.endswith(\".pdf\"):\n",
    "            path_list.append(filename)\n",
    "    \n",
    "    return path_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getTextO is the original method to extract text using PyPDF, it is less accurate but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextO(pdf):\n",
    "    try:\n",
    "        pdf_document = PyPDF2.PdfFileReader(mypdf)\n",
    "        full_text = \"\"\n",
    "        pages = pdf_document.numPages\n",
    "        print(pdf_document.numPages) \n",
    "        print(\"\\n\")\n",
    "    \n",
    "        for num in range(0, pages):\n",
    "            nth_page = pdf_document.getPage(num)\n",
    "        ##print(nth_page.extractText())\n",
    "        ##print(\"\\n\")\n",
    "        ##print(\"PAGE: \" + str(num))\n",
    "        ##print(nth_page.getContents())\n",
    "        ##print(\"\\n\")\n",
    "        ##print(\"PAGE: \" + str(num))\n",
    "            full_text = full_text + nth_page.extractText()\n",
    "        return full_text\n",
    "    except Exception:\n",
    "        return \"Text For This File Could Not be Read\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter path to start extracting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = {}\n",
    "count = 0\n",
    "\n",
    "file_names = getPaths(\"ENTER HERE\")\n",
    "for directory in directories:\n",
    "    mypdf = open(directory, mode='rb')\n",
    "    print(\"Document number \" + str(count) + \" out of 109\")\n",
    "    fulltext[file_names[count]] = getTextO(mypdf)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of the articles are extracted using the rough PyPDF library, checkwordsize makes sure to assure quality, resorting to the OCR use if the quality is not met since text in the PyPDF method is not tokenized\n",
    "\n",
    "However, the step above can be skiped and the useOCR method can be the only method if enough processing power is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n",
    "def checkwordsize(word_list):\n",
    "    word_split_proper = True\n",
    "    for word in word_list:\n",
    "        if len(word) > 50:\n",
    "            word_split_proper = False\n",
    "            break\n",
    "    return word_split_proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useOCR(article):\n",
    "    directory_name = \"C:\\\\Users\\\\Tejas\\\\Desktop\\\\UCSF Internship\\\\UCSF-Internship\\\\WordFinder\\\\PDFstore\\\\\" + article\n",
    "    try:\n",
    "        pages = pdf2image.convert_from_path(pdf_path=directory_name, dpi=200, size=(1654,2340))\n",
    "        full_text = \"\"\n",
    "        for i in range(len(pages)):\n",
    "            pages[i].save('Images\\\\imagenum' + str(i) + '.jpg')\n",
    "        for i in range(len(pages)):\n",
    "            content = pt.image_to_string(pages[i], lang='eng')\n",
    "            full_text = full_text + \" \" + content\n",
    "        return full_text\n",
    "    except Exception:\n",
    "        return \"BAD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does what was described in the above documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_words = {}\n",
    "for article in fulltext:\n",
    "    article_text = TextBlob(fulltext[article])\n",
    "    article_text = article_text.lower()\n",
    "    ##print(article_text.words)\n",
    "    allow = checkwordsize(article_text.words)\n",
    "    if allow:\n",
    "        article_words[article] = article_text.words\n",
    "    else:\n",
    "        print(\"Using OCR for \" + article)\n",
    "        article_words[article] = TextBlob(useOCR(article)).words\n",
    "    print(\"Finished article \" + article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fulltextinwordsOCR.json', 'w') as json_file:\n",
    "  json.dump(article_words, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main methods to try to find extract the relationship of immport\n",
    "\n",
    "Each article is searched for the keyword \"immport.\" Once immport is found, another method is called to find and store the 30 words before and after it (this delta can be changed). Finally those 30 words before and after are saved in another dictionary to be analyzed later and find the exact usage of immport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_one_article(test):\n",
    "\n",
    "    use_list = []\n",
    "    count = 0\n",
    "    use_count = 0\n",
    "\n",
    "    keyword = \"immport\"\n",
    "\n",
    "    for count in range(len(test)):\n",
    "        if test[count].lower() == keyword:\n",
    "           ## print(\"found \" + keyword)\n",
    "            use_count = use_count + 1\n",
    "            ##print(\"Use number: \" + str(use_count) + \"\\n\")\n",
    "            use_list.append(print_sentence(max(0, count - 30), min(len(test) - 1, count + 30), test))\n",
    "            ##print(\"\\n\")\n",
    "            count = count + 60\n",
    "        else:\n",
    "            count = count + 1\n",
    "            \n",
    "    return use_list\n",
    "\n",
    "def visualize_use(min, max, word_list, freq_dict):\n",
    "    local = min\n",
    "    \n",
    "    while local <= max:\n",
    "        if freq_dict.get(word_list[local]):\n",
    "            freq_dict[word_list[local]] = freq_dict.get(word_list[local]) + 1\n",
    "        else:\n",
    "            freq_dict[word_list[local]] = 1\n",
    "        local = local + 1\n",
    "    \n",
    "    return freq_dict\n",
    "\n",
    "\n",
    "def find_freqs(test):\n",
    "    \n",
    "    global_freq = {}\n",
    "    count = 0\n",
    "    keyword = \"immport\"\n",
    "    \n",
    "    for count in range(len(test)):\n",
    "        if test[count].lower() == keyword:\n",
    "            global_freq = visualize_use(max(0, count - 30), min(len(test) - 1, count + 30), test, global_freq)\n",
    "            count = count + 60\n",
    "        else:\n",
    "            count = count + 1\n",
    "            \n",
    "    return global_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the actual sentences and stores them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uses_of_keyword = {}\n",
    "\n",
    "for key in article_words:\n",
    "    current = article_words[key]\n",
    "    use_list = search_one_article(current)\n",
    "    print(use_list)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")    \n",
    "    uses_of_keyword[key] = use_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dictionary of frequencies of each word that is near the use of the word immport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_freq_list = {}\n",
    "\n",
    "for key in article_words:\n",
    "    current = article_words[key]\n",
    "    ##print(current)\n",
    "    one_freq_list = find_freqs(current)\n",
    "    print(one_freq_list)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")    \n",
    "    global_freq_list[key] = one_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('use_by_freq_OCR.json', 'w') as json_file:\n",
    "  json.dump(global_freq_list, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds and details the exact structured output for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irg_list = []\n",
    "genes_tracker = []\n",
    "gene_expression_list = []\n",
    "secondary_analysis_list = []\n",
    "gene_analysis_list = []\n",
    "galaxy_list = []\n",
    "data_reuse_list = []\n",
    "cytokine_registry_list = []\n",
    "hla_list = []\n",
    "flock_list = []\n",
    "metacyto_list = []\n",
    "primary_analysis_list = []\n",
    "fcs_list = []\n",
    "rimmport_list = []\n",
    "\n",
    "def add_to_dict(freq_list, main_dict, article):\n",
    "    \n",
    "    cytokine = False\n",
    "    registry = False\n",
    "    \n",
    "    irg = False\n",
    "    ir = False\n",
    "    data = False\n",
    "    reuse = False\n",
    "    \n",
    "    fcs = False\n",
    "    fcsTrans = False\n",
    "    \n",
    "    flock = False\n",
    "    \n",
    "    galaxy = False\n",
    "    \n",
    "    gene = False\n",
    "    expression = False\n",
    "    list_ = False\n",
    "    \n",
    "    hla = False\n",
    "    \n",
    "    cyto = False\n",
    "    meta = False\n",
    "    metacyto = False\n",
    "    \n",
    "    primary = False\n",
    "    secondary = False\n",
    "    analysis = False\n",
    "    \n",
    "    rimmport = False\n",
    "    \n",
    "    at_least_one  = False\n",
    "    \n",
    "    immport = False\n",
    "    \n",
    "    for word in freq_list:\n",
    "        if \"IRG\" == word or \"IRGs\" == word:\n",
    "            irg = True\n",
    "            at_least_one  = True\n",
    "        if \"immune-related\" == word.lower():\n",
    "            ir = True\n",
    "        if \"cytokine\" in word.lower():\n",
    "            cytokine = True\n",
    "            at_least_one  = True\n",
    "        if \"registry\" in word.lower():\n",
    "            registry = True\n",
    "            at_least_one  = True            \n",
    "        if \"data\" in word.lower():\n",
    "            data = True\n",
    "            at_least_one  = True\n",
    "        if \"reuse\" == word.lower():\n",
    "            reuse = True\n",
    "            at_least_one  = True\n",
    "        if \"fcs\" in word.lower():\n",
    "            fcs = True\n",
    "            at_least_one  = True\n",
    "        if \"fcstrans\" in word.lower():\n",
    "            fcsTrans = True\n",
    "            at_least_one  = True\n",
    "        if \"flock\" in word.lower():\n",
    "            flock = True\n",
    "            at_least_one  = True\n",
    "        if \"galaxy\" == word.lower():\n",
    "            galaxy = True\n",
    "            at_least_one  = True\n",
    "        if \"gene\" or \"genes\" in word.lower():\n",
    "            gene = True\n",
    "            at_least_one  = True\n",
    "        if \"list\" in word.lower():\n",
    "            list_ = True\n",
    "            at_least_one  = True\n",
    "        if \"expression\" in word.lower():\n",
    "            expression = True\n",
    "            at_least_one  = True\n",
    "        if \"hla\" == word.lower():\n",
    "            hla = True\n",
    "            at_least_one  = True\n",
    "        if \"cyto\" == word.lower():\n",
    "            cyto = True\n",
    "            at_least_one  = True\n",
    "        if \"meta\" == word.lower():\n",
    "            meta = True\n",
    "            at_least_one  = True\n",
    "        if \"metacyto\" == word.lower():\n",
    "            metacyto = True\n",
    "            at_least_one  = True\n",
    "        if \"primary\" == word.lower():\n",
    "            primary = True\n",
    "            at_least_one  = True\n",
    "        if \"secondary\" == word.lower():\n",
    "            secondary = True\n",
    "            at_least_one  = True\n",
    "        if \"analysis\" == word.lower():\n",
    "            analysis = True\n",
    "            at_least_one  = True\n",
    "        if \"rimmport\" in word.lower():\n",
    "            rimmport = True\n",
    "            at_least_one  = True\n",
    "        if \"immport\" in word.lower():\n",
    "            immport = True\n",
    "\n",
    "    if irg or (ir and gene):\n",
    "        if \"IRG\" in main_dict:\n",
    "            main_dict[\"IRG\"] = main_dict[\"IRG\"] + 1  \n",
    "        else:\n",
    "            main_dict[\"IRG\"] = 1\n",
    "    if galaxy:\n",
    "        if \"galaxy\" in main_dict:\n",
    "            main_dict[\"galaxy\"] = main_dict[\"galaxy\"] + 1\n",
    "        else: \n",
    "            main_dict[\"galaxy\"] = 1\n",
    "    if fcsTrans:\n",
    "        if \"fcstrans\" in main_dict:\n",
    "            main_dict[\"fcstrans\"] = main_dict[\"fcstrans\"] + 1\n",
    "        else: \n",
    "            main_dict[\"fcstrans\"] = 1\n",
    "    if fcs:\n",
    "        if \"fcs\" in main_dict:\n",
    "            main_dict[\"fcs\"] = main_dict[\"fcs\"] + 1\n",
    "        else: \n",
    "            main_dict[\"fcs\"] = 1\n",
    "    if hla:\n",
    "        if \"hla\" in main_dict:\n",
    "            main_dict[\"hla\"] = main_dict[\"hla\"] + 1\n",
    "        else: \n",
    "            main_dict[\"hla\"] = 1\n",
    "    if flock:\n",
    "        if \"flock\" in main_dict:\n",
    "            main_dict[\"flock\"] = main_dict[\"flock\"] + 1\n",
    "        else: \n",
    "            main_dict[\"flock\"] = 1\n",
    "    if rimmport:\n",
    "        if \"rimmport\" in main_dict:\n",
    "            main_dict[\"rimmport\"] = main_dict[\"rimmport\"] + 1\n",
    "        else: \n",
    "            main_dict[\"rimmport\"] = 1\n",
    "    if metacyto:\n",
    "        if \"metacyto\" in main_dict:\n",
    "            main_dict[\"metacyto\"] = main_dict[\"metacyto\"] + 1\n",
    "        else: \n",
    "            main_dict[\"metacyto\"] = 1\n",
    "    \n",
    "    print(\"DATA FOR ARTICLE \" + article + \"\\n\")\n",
    "    \n",
    "    at_least_one  = False\n",
    "        \n",
    "    if cytokine and registry:\n",
    "        print(\"Uses Cytokine Registry\\n\")\n",
    "        cytokine_registry_list.append(article)\n",
    "        if \"cytokine_registry\" in main_dict:\n",
    "            main_dict[\"cytokine_registry\"] = main_dict[\"cytokine_registry\"] + 1\n",
    "        else: \n",
    "            main_dict[\"cytokine_registry\"] = 1\n",
    "        at_least_one  = True\n",
    "    if data and reuse:\n",
    "        print(\"Uses Data Reuse\\n\")\n",
    "        data_reuse_list.append(article)\n",
    "        if \"data_reuse\" in main_dict:\n",
    "            main_dict[\"data_reuse\"] = main_dict[\"data_reuse\"] + 1\n",
    "        else: \n",
    "            main_dict[\"data_reuse\"] = 1\n",
    "        at_least_one  = True\n",
    "    if fcs or fcsTrans:\n",
    "        fcs_list.append(article)\n",
    "        print(\"Uses FCSTrans\\n\")\n",
    "        at_least_one  = True\n",
    "    if flock:\n",
    "        flock_list.append(article)\n",
    "        print(\"Uses FLOCK\\n\")\n",
    "        at_least_one  = True\n",
    "    if galaxy:\n",
    "        galaxy_list.append(article)\n",
    "        print(\"Uses Galaxy\\n\")\n",
    "        at_least_one  = True\n",
    "    if gene and expression:\n",
    "        gene_expression_list.append(article)\n",
    "        print(\"Uses Gene Expression\\n\")\n",
    "        if \"gene_expression\" in main_dict:\n",
    "            main_dict[\"gene_expression\"] = main_dict[\"gene_expression\"] + 1\n",
    "        else: \n",
    "            main_dict[\"gene_expression\"] = 1\n",
    "        at_least_one  = True\n",
    "    if gene and list_:\n",
    "        genes_tracker.append(article)\n",
    "        print(\"Uses Gene List\\n\")\n",
    "        if \"gene_list\" in main_dict:\n",
    "            main_dict[\"gene_list\"] = main_dict[\"gene_list\"] + 1\n",
    "        else: \n",
    "            main_dict[\"gene_list\"] = 1\n",
    "        at_least_one  = True\n",
    "    if gene and analysis:\n",
    "        gene_analysis_list.append(article)\n",
    "        print(\"Uses Gene Analysis\")\n",
    "        if \"gene_analysis\" in main_dict:\n",
    "            main_dict[\"gene_analysis\"] = main_dict[\"gene_analysis\"] + 1\n",
    "        else: \n",
    "            main_dict[\"gene_analysis\"] = 1\n",
    "        at_least_one  = True\n",
    "    if gene:\n",
    "        print(\"Uses Gene\")\n",
    "        at_least_one  = True\n",
    "    if hla:\n",
    "        hla_list.append(article)\n",
    "        print(\"Uses HLA\\n\")\n",
    "        at_least_one  = True\n",
    "    if (cyto and meta) or metacyto:\n",
    "        metacyto_list.append(article)\n",
    "        print(\"Uses MetaCyto\\n\")\n",
    "        at_least_one  = True\n",
    "    if primary and analysis:\n",
    "        print(\"Uses Primary Analysis\\n\")\n",
    "        primary_analysis_list.append(article)\n",
    "        if \"primary_analysis\" in main_dict:\n",
    "            main_dict[\"primary_analysis\"] = main_dict[\"primary_analysis\"] + 1\n",
    "        else: \n",
    "            main_dict[\"primary_analysis\"] = 1\n",
    "        at_least_one  = True\n",
    "    if secondary and analysis:\n",
    "        secondary_analysis_list.append(article)\n",
    "        print(\"Uses Secondary Analysis\\n\")\n",
    "        if \"secondary_analysis\" in main_dict:\n",
    "            main_dict[\"secondary_analysis\"] = main_dict[\"secondary_analysis\"] + 1\n",
    "        else: \n",
    "            main_dict[\"secondary_analysis\"] = 1\n",
    "        at_least_one  = True\n",
    "    if rimmport:\n",
    "        rimmport_list.append(article)\n",
    "        print(\"Uses RImmPort\\n\")\n",
    "        at_least_one  = True\n",
    "    if irg or (ir and gene):\n",
    "        irg_list.append(article)\n",
    "        print(\"Uses IRGs\")\n",
    "        at_least_one = True\n",
    "    \n",
    "    return main_dict\n",
    "\n",
    "usage_indicator_dict = {}\n",
    "\n",
    "for article in global_freq_list:\n",
    "    usage_indicator_dict = add_to_dict(global_freq_list[article], usage_indicator_dict, article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives the exact number of uses for each of the structured output\n",
    "\n",
    "Here is a sample output:\n",
    "    \n",
    " {'gene_list': 30,\n",
    " 'gene_analysis': 54,\n",
    " 'secondary_analysis': 6,\n",
    " 'gene_expression': 35,\n",
    " 'galaxy': 2,\n",
    " 'hla': 3,\n",
    " 'flock': 4,\n",
    " 'rimmport': 4,\n",
    " 'metacyto': 4,\n",
    " 'cytokine_registry': 2,\n",
    " 'data_reuse': 7,\n",
    " 'primary_analysis': 7,\n",
    " 'IRG': 32,\n",
    " 'fcs': 4}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_indicator_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates table and later the visual graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "keys = usage_indicator_dict.keys()\n",
    "\n",
    "number = []\n",
    "key_values = []\n",
    "for key in keys:\n",
    "    key_values.append(key)\n",
    "    number.append(usage_indicator_dict[key])\n",
    "df = pd.DataFrame({'use_cases': key_values , 'val': number})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.bar(x='use_cases', y='val', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation to check where immport uses were not found in any article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "not_found = []\n",
    "for article in global_freq_list:\n",
    "    if find_use(global_freq_list[article], article) == 1:\n",
    "        print(\"Not found on \" + article)\n",
    "        not_found.append(article)\n",
    "        count = count + 1\n",
    "    \n",
    "print(\"Num not found: \" + str(count))\n",
    "print(not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final results of all the uses and the articles under them. \n",
    "\n",
    "OCRScrape.ipynb also includes more detailed information on some of the specific words and topics of the articles if that is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expression_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_analysis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_analysis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reuse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cytokine_registry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metacyto_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_analysis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rimmport_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
