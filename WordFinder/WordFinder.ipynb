{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import NLP Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = TextBlob(\"Immunology researchers are beginning to explore the possibilities of reproducibility, reuse and secondary analyses of immunology data. Open-access datasets are being applied in the validation of the methods used in the original studies, leveraging studies for meta-analysis, or generating new hypotheses. To promote these goals, the ImmPort data repository was created for the broader research community to explore the wide spectrum of clinical and basic research data and associated findings. The ImmPort ecosystem consists of four components–Private Data, Shared Data, Data Analysis, and Resources—for data archiving, dissemination, analyses, and reuse. To date, more than 300 studies have been made freely available through the Shared Data portal (www.immport.org/immport-open), which allows research data to be repurposed to accelerate the translation of new insights into discoveries.\")\n",
    "article_text = article_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['immunology', 'researchers', 'are', 'beginning', 'to', 'explore', 'the', 'possibilities', 'of', 'reproducibility', 'reuse', 'and', 'secondary', 'analyses', 'of', 'immunology', 'data', 'open-access', 'datasets', 'are', 'being', 'applied', 'in', 'the', 'validation', 'of', 'the', 'methods', 'used', 'in', 'the', 'original', 'studies', 'leveraging', 'studies', 'for', 'meta-analysis', 'or', 'generating', 'new', 'hypotheses', 'to', 'promote', 'these', 'goals', 'the', 'immport', 'data', 'repository', 'was', 'created', 'for', 'the', 'broader', 'research', 'community', 'to', 'explore', 'the', 'wide', 'spectrum', 'of', 'clinical', 'and', 'basic', 'research', 'data', 'and', 'associated', 'findings', 'the', 'immport', 'ecosystem', 'consists', 'of', 'four', 'components–private', 'data', 'shared', 'data', 'data', 'analysis', 'and', 'resources—for', 'data', 'archiving', 'dissemination', 'analyses', 'and', 'reuse', 'to', 'date', 'more', 'than', '300', 'studies', 'have', 'been', 'made', 'freely', 'available', 'through', 'the', 'shared', 'data', 'portal', 'www.immport.org/immport-open', 'which', 'allows', 'research', 'data', 'to', 'be', 'repurposed', 'to', 'accelerate', 'the', 'translation', 'of', 'new', 'insights', 'into', 'discoveries'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter keywords you want in the article\n",
      "To stop type: \"stop\" \tasdfhsdfkl\n",
      "Enter keywords you want in the article\n",
      "To stop type: \"stop\" \tconsists of\n",
      "Enter keywords you want in the article\n",
      "To stop type: \"stop\" \tStoP\n"
     ]
    }
   ],
   "source": [
    "keyword_list\n",
    "stop = False\n",
    "\n",
    "while(not stop):\n",
    "    keyword = input('Enter keywords you want in the article\\nTo stop type: \"stop\" \\t')\n",
    "    if(keyword.lower() == \"stop\"):\n",
    "        stop = True;\n",
    "    else:\n",
    "        keyword_list.append(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchForKeyword(keyword):\n",
    "    word_blob = TextBlob(keyword.lower())\n",
    "    nGram = len(word_blob.words)\n",
    "    \n",
    "    if nGram == 0:\n",
    "        print(\"No keywords given!\")\n",
    "    elif nGram == 1:\n",
    "        article_list = article_text.words\n",
    "        for word in article_list:\n",
    "            if word == keyword.lower():\n",
    "                return True\n",
    "    else:\n",
    "        article_list = article_text.ngrams(n = nGram)\n",
    "        for ngram in article_list:\n",
    "            if article_list == word_blob.words:\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is (are) the articles that match your query:\n",
      "immunology researchers are beginning to explore the possibilities of reproducibility, reuse and secondary analyses of immunology data. open-access datasets are being applied in the validation of the methods used in the original studies, leveraging studies for meta-analysis, or generating new hypotheses. to promote these goals, the immport data repository was created for the broader research community to explore the wide spectrum of clinical and basic research data and associated findings. the immport ecosystem consists of four components–private data, shared data, data analysis, and resources—for data archiving, dissemination, analyses, and reuse. to date, more than 300 studies have been made freely available through the shared data portal (www.immport.org/immport-open), which allows research data to be repurposed to accelerate the translation of new insights into discoveries.\n"
     ]
    }
   ],
   "source": [
    "found = False\n",
    "found_list = [] #to be implemented later\n",
    "count = 0\n",
    "\n",
    "#only searches for article right now, if there are multiple articles this would be a subroutine\n",
    "while(not found and count < len(keyword_list)):\n",
    "    found = searchForKeyword(keyword_list[count])\n",
    "    count = count + 1\n",
    "\n",
    "if found:\n",
    "    print(\"Here is (are) the articles that match your query:\")\n",
    "    print(article_text)\n",
    "else:\n",
    "    print(\"Nothing found with the query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "words = open(\"words-by-frequency.txt\").read().split()\n",
    "wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "maxword = max(len(x) for x in words)\n",
    "\n",
    "def infer_spaces(s):\n",
    "    \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "    without spaces.\"\"\"\n",
    "\n",
    "    # Find the best match for the i first characters, assuming cost has\n",
    "    # been built for the i-1 first characters.\n",
    "    # Returns a pair (match_cost, match_length).\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "        return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "    # Build the cost array.\n",
    "    cost = [0]\n",
    "    for i in range(1,len(s)+1):\n",
    "        c,k = best_match(i)\n",
    "        cost.append(c)\n",
    "\n",
    "    # Backtrack to recover the minimal-cost string.\n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i>0:\n",
    "        c,k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i-k:i])\n",
    "        i -= k\n",
    "\n",
    "    return \" \".join(reversed(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
